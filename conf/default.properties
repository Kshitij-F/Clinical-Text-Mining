###########################################################3333333333
############# UMLS term matching configuration #####################3
# jdbcDriver is the database url that uses for extern info for a term in UMLS. e.g. selecting TUI by CUI from the table MRSTY.
# for now, table mrstr is neccessary
jdbcDriver=jdbc:mysql://localhost:3306/umls?user=root&password=root
#url of solr we use to match umls term. do not used solr by default
solrServerUrl=http://localhost:8983/solr

# caseFactor is [0, 1] value. It indicates how much you concern the case. It will affect the score
# when you select a term from solr. Value 0 means upcase and lowcase are totally different, and
# value 1 means upcase and lowcase are not different at all.
caseFactor=0.8

#Should we take the newline as the end of a sentence? or just ignore the newline?  not used for now
#  1: replace with space; 2: replace with '.'; 0: do nothing
ignoreNewLine=2

#######################################################################
########## data source to fetching configuration ######################
# how to get the text to get Ngram; the blogId will select as distict, and the blogTextCol will be limit to 1 row.
blogDbUrl=jdbc:mysql://localhost:3306/ytex?user=root&password=root
blogTbl=tmp_org_yahoo
#blogTbl=content_org_new
blogIdCol=id
#blogIdCol=blogId
blogTextCol=concat(subject, ". ", content, ". ", chosenanswer)
#blogTextCol=text_content

# limit the blog to be analyzed, mainly for test
blogLimit=200

#target term info in database
targetTermTbl=_target_term_
targetTermTblDropAndCreate=true
# if true, using solr for matching a ngram with target terms, else using database query for matching
targetTermUsingSolr=false

#######################################################################
################### NLP relative configuration ###############################
#root dir of lvg
lvgdir=C:\\fsu\\ra\\UmlsTagger\\lvg2015\\

# include POS tagger. The Ngram (basic terms) have to contain at least one of those POS tagger. it also the definition of 'noun' in this tool. No filter if empty
#posInclusive=NN NNS NNP NNPS
posInclusive=
# 0 - 100. if the similarity score for a ngram is greater than this threshold, the ngran will be consider as umls term
umlsLikehoodLimit=30
# the window length to fetch context of a ngram
WinLen=10

# use to force delimit gram. Delimiter = Pattern.compile("[,;/\\:\\(\\)\\[\\]\\{\\}\"]+")
delimiter =[,;/\\:\\(\\)\\[\\]\\{\\}\"]+

# how does ngram  match the stop words list? 0:exactly matching; 1: ngram contains any stop word; 2: ngram start or end with any stop word; others: no filter
stopwordMatchType=2

# besides the file of stop word, you can specify a regex to indicate what is a stop word.
# exclude the gram start or end with digit. (remove the matched item)
# exclude words only start or end with one letter
# stopwordRegex=^\\d+.*|.*\\d$|^\\S(\\s.*|$)|(^|.*\\s)\\S    ----- for clustering
stopwordRegex=aaaaaaaaaaaaaaaaaaaaaaaa
# pos tagger filter (remove the matched item). 1: no noun; 2: ^N+P+N 3: not end with N
#posFilterRegex=[^N]* [^N]*PN .*[^N]$
posFilterRegex=[^N]*

#######################################################################
############### Ngram relative configuration ###################################
# the threshold of tf when fetch ngram in partition
partitionTfFilter=2
# the threshold of tf when fetch ngram in first stage
stag1TfFilter=2
stag1CvalueFilter=1
# the threshold of tf when fetch ngram in second stage
stag2TfFilter=10
stag2CvalueFilter=1
# the thresholh of umls/chv score. no filter if it is -1
stag2UmlsScoreFilter= 30
stag2ChvScoreFilter=-1

# if any of this greater than 0, it will rank the ngram, and take the top N.
# this configuration can affect bags of words function.
topTfNgram=0
topCvalueNgram=0
topTfdfNgram=0

######################## bags of words configuration ###############
bagsOfWord=false
bagsOfWordFilter=true
bowTopNgram=10000
######################## end of bags of words configuration ######

#######################################################################
############# Clustering relative configuration ##########################
# Nlp do not allow multi-thread, so you can not use local[N] for generating Ngram, but you can use it to run kmeans
sparkMaster=local
partitionNumber=2

#####*_*####get the training data from (previous save) file, do not construct the Ngram again.
clusteringFromFile=false
ngramSaveFile=c:\\fsu\\ra\\data\\ngram_yahoo_0211.serd.no_bow

########### only use chv term as trainig data
trainOnlyChv=false
# filter the ngran before run kmeans (remove the matched item)
trainedNgramFilterPosRegex=[^N]*PN
# how many percent of the data is sample as test data(for evaluation), <= 0, no thing is test
testSample=0
sampleRuns=1
#number of ngram for training. For test purpose. <0: no limit;
trainNgramCnt=-1

####### if normalized the feature to [0,1] range. see https://en.wikipedia.org/wiki/Feature_scaling
# Rescaling or Standardization
normalizeFeature=true
#rescale to [0,1]
normalize_rescale=true
# the same as z-score. Be sure you know what it affects if you try to use it
normalize_standardize=false
# if the z-score of a value is grater than this factor, it is considered as outlier and will be regular to the value to whose z-score is this factor.
#like a lower boundary and upper boundary
normalize_outlier_factor=2
# generate feature vectors only. So you can use the vectors in R or other tools.
outputVectorOnly=false
# PCA only. Compact the feature space matrix to a N dimensions space using PCA. <=0, do nothing.
pcaDimension=0


# the feature used in kmeans
#           tfdf,cvalue,umls_score,chv_score,contain_umls,contain_chv,win_umls,win_chv,sent_umls,sent_chv,umls_dist,chv_dist,nn,an,pn,anpn,stys,win_pos,capt_first,capt_all,capt_term
useFeatures4Train=tfdf:0,tf,df,cvalue:1,umls_score:0.5,chv_score:0,contain_umls:1,contain_chv:1,nn,an,pn,anpn,stys:0,win_pos,capt_first,capt_all,capt_term,prefix:1,suffix:1,win_umls,win_chv,sent_umls,sent_chv,umls_dist,chv_dist
useFeatures4Test =tfdf:0,tf,df,cvalue:1,umls_score:0.5,chv_score:0,contain_umls:1,contain_chv:1,nn,an,pn,anpn,stys:0,win_pos,capt_first,capt_all,capt_term,prefix:1,suffix:1,win_umls,win_chv,sent_umls,sent_chv,umls_dist,chv_dist
#useFeatures4Train=tfdf,cvalue,umls_score,chv_score,contain_umls,contain_chv,nn,an,pn,anpn,stys,win_pos,capt_first,capt_all,capt_term,prefix,suffix,win_umls,win_chv,sent_umls,sent_chv,umls_dist,chv_dist
#useFeatures4Test =tfdf,cvalue,umls_score,chv_score,contain_umls,contain_chv,nn,an,pn,anpn,stys,win_pos,capt_first,capt_all,capt_term,prefix,suffix,win_umls,win_chv,sent_umls,sent_chv,umls_dist,chv_dist


# the top semantic type we make it as features
semanticType=T033,T121,T061,T047,T109,T023,T184,T074,T116,T123,T059,T046
# the syntax that occur around a ngram in a window. they have been transformed to a single character
posInWindown=CMDEFPANURTVO
# prefix/suffix use window or only process ngram itself
prefixSuffixUseWindow=false
tfdfLessLog=false

###### k-mean parameters #######
# if run k-mean or not
runKmeans=false
# the start/end/step point of the k (cluster number)
k_start=20
k_end=20
k_step=1
# the maximum of iteration of the k-mean algorithm if it is not convergent
maxIterations=1000
# run the following number of times for every k, and take the least cost one
runs=10
# if remove these clusters that contain too small number of points.
reviseModel=true
#################################
# how many percent of the training data does a cluster at least contain (compare to the average number of ngram in a cluster. ?
clusterThresholdPt=10
clusterThresholdLimit=3
# the number of point sampled to calculate the average distance of point to centers
clusterThresholSample=500
# if (the distance of a center to other centers) / (average distance of a point to a center) > (this factor), this center will not be discard.
clusterThresholFactor=3
# get the score for every K(number of cluster), and then we can choose a 'best' K.
#see:https://en.wikipedia.org/wiki/Silhouette_(clustering)
clusterScore=false
#######################################
### Ranking relative configuration ###
# get the baseline result( rank on tfall and cvlaue)
baseLineRank=true
runRank=true
# the granular of rank (percent), e.g. rankGranular=5 work as 5%, 10%, 15%,
rankGranular=5
# since rank are based on percentage of ngram, this parameter specify the base number to calculate the percentage.
# percentage = (# of current ngram)/rankLevelBase, -1: use number of tested ngram if testSample>0, or use number of all ngram
rankLevelBase=-1
# specify how many level of percentage, e.g. 4 means 5%,10%,15%,20%, the percentage may be greater than 100%
rankLevelNumber=40

# the beta value for evaluating f-score, see:https://en.wikipedia.org/wiki/F1_score
# Two other commonly used F measures are the F_{2} measure, which weights recall higher than precision,
#and the F_{0.5} measure, which puts more emphasis on precision than recall.
fscoreBeta=0.5

# ranking with the training data, mainly to evaluatation
rankWithTrainData=false

#######################################################################
############### Output configuration ##################################
#show original ngram before training
showOrgNgramNum=100
showSentence=true
# save the above showing ngram to file
saveNgram2file=c:/fsu/ra/orgGram.txt
# shown ngram filter based on N
showOrgNgramOfN=1,2,3,4,5
# shown ngram filter based on pos tagger
showOrgNgramOfPosRegex=.*
# shown ngram filter based on text
showOrgNgramOfTextRegex=.*
# show the number of ngram in every cluster. <0, show nothing
showNgramInCluster=0
#show the average and standard deviation of tf in clusters. Not configurable, always true
#showTfAvgSdInCluster=true
#how many percent of ngram is shown the detail after rank. it show info of every ngram in this top ${showDetailRankPt} percent; <0 don't show detail;
showDetailRankPt=0
# if a Ngram math this filter(regex), the detail information will output to console..
debugFilterNgram=aaaaaaaaaaaaaaaaaa
