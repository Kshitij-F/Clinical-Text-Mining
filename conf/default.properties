
############# UMLS term matching configuration #####################3
# jdbcDriver is the database url that uses for extern info for a term in UMLS. e.g. selecting TUI by CUI from the table MRSTY.
# for now, table mrconso, mrstr is neccessary. You have to specify the database of the umls;
jdbcDriver=jdbc:mysql://localhost:3306/umls?user=root&password=root

#######################################################################
########## data source to fetching text to analyzing in CHV paper ######################
# how to get the text to generate Ngram; the blogId will select as distict, and the blogTextCol will be limit to 1 row.
blogDbUrl=jdbc:mysql://localhost:3306/ytex?user=root&password=root
blogTbl=tmp_org_yahoo
#blogTbl=content_org_new
blogIdCol=id
#blogIdCol=blogId
blogTextCol=concat(subject, ". ", content, ". ", chosenanswer)
#blogTextCol=text_content
# limit the blog to be analyzed, mainly for test
blogLimit=100

#####################################################################################
################################# fuzzy matching configuration ######################
##### You can use Solor or Mysql as the index search server. You have to initilize at least one of them
##### Use database is more easy to configure. But it may be a little slower.
# if true, using solr for matching a ngram with target terms, else using database query for matching
targetTermUsingSolr=True
#target term info in database
sourceTermTbl=umls.mrconso
targetTermTbl=_target_term_
targetTermTblDropAndCreate=false
# the sql after keyword 'where' that you want to push into the index server.
# e.g. select cui,aui,sab,str from umls.mrconso where 1=1 limit 1000 ;
sourceTermQueryOption= LAT = 'ENG'

#url of solr we use to match umls term. do not used solr by default. You also can use database to query.
# it depend on the configuration
solrServerUrl=http://localhost:8983/solr

# ===================== Cache config =======================================
# if the 'memcached' is empty, use ehCache by default; if set, a memcached server should be started up in advance.
# support multiple server: 1.1.1.1:11211,1.1.1.2:11211
memcached=
ehCacheEntities=5000
#===========================================================================


# caseFactor is [0, 1] value. It indicates how much you concern the case. It will affect the similarity score
# when you select a term from solr. Value 0 means upcase and lowcase are totally different, and
# value 1 means upcase and lowcase are not different at all.
caseFactor=0.9
# if a cui string is more close to the prefer term string, it will get higher score.
preferStrFactor=0.1

#######################################################################
################### NLP relative configuration ########################
#root dir of lvg. Use stanford nlp is recommended.
useStanfordNLP=true
annotators="tokenize, ssplit, pos, lemma, ner,parse,depparse"
tokenize_options=tokenizeNLs=true,invertible=true
stanfordTaggerOption=model=edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger
stanfordPatternFile=C:\\fsu\\ra\\UmlsTagger\\conf\\pattern_cancer.txt
# use the dependcy tree to find terms before using the syntactic tree.
useDependencyTree=true
partUmlsTermMatch=false
ssplit_newlineIsSentenceBreak=always
# for pattern parsing. Store the non-UMLS term and output then to a file.
analyzNonUmlsTerm = true
# the maximum length of a sentence. (It is number of word since Feb. 23th 2017. Before that is the number of characters)
sentenceLenMax=100
# split the text into block before sentence segmentation. For clinical trials
textBlockDelimiter=#|\\n
# Special segmentation of the text before sentence segmentation (':', ' - ', 'Or|OR', 'No', 'At'). For clinical trials
textBlockDelimiterSpecialEnable=true

# include POS tagger. The Ngram (basic terms) have to contain at least one of those POS tagger. it also the definition of 'noun' in this tool. No filter if empty
#posInclusive=NN NNS NNP NNPS
posInclusive=
# 0 - 100. if the similarity score for a ngram is greater than this threshold, the ngran will be consider as umls term
umlsLikehoodLimit=80
# the window length to fetch context of a ngram
WinLen=10
#max length of ngram
ngramN=5
# if store sentence in ngram
ngramKeepSentence=true

# use to force delimit gram. Delimiter = Pattern.compile("[,;/\\:\\(\\)\\[\\]\\{\\}\"]+")
delimiter =[,;/\\:\\(\\)\\[\\]\\{\\}\"]+

# how does ngram  match the stop words list? 0:exactly matching; 1: ngram contains any stop word; 2: ngram start or end with any stop word; others: no filter
stopwordMatchType=2
# besides the file of stop word, you can specify a regex to indicate what is a stop word.
# exclude the gram start or end with digit. (remove the matched item)
# exclude words only start or end with one letter
stopwordRegex=^\\d+.*|.*\\d$|^\\S(\\s.*|$)|(^|.*\\s)\\S
#stopwordRegex=aaaaaaaaaaaaaaaaaaaaaaaa
# pos tagger filter (remove the matched item). 1: no noun; 2: ^N+P+N 3: not end with N
posFilterRegex=[^N]* [^N]*PN .*[^NAG]$
# have to be ended with noun
#posFilterRegex=.*[^N]$
# a regex to check a string as a whole (may be several words) should query for a CUI or not
#(different form stopwords check since stop word checks every word in a string)
# 1. no a-z; 2. started or ended a word without a-z
cuiStringFilterRegex=[^a-zA-Z]*|^[^a-zA-Z]+\\s.*|.*\\s[^a-zA-Z]+$

#######################################################################
############### Ngram relative configuration (CHV paper) ##############
# the threshold of tf when fetch ngram in partition
partitionTfFilter=2
# when reach this number of ngram in this partition, start to reduce ngram
partitionReduceStartPoint=100000
# each time this number of new ngram in this partition, after start point, reduce ngram
partitionReduceStartStep=10000
# At lease try to reduce how many ngram, fraction of 'stage1ReduceStartStep'
partitionReduceFraction=0.1

# the threshold of tf when fetch ngram in first stage
stag1TfFilter=2
stag1CvalueFilter=1
# the threshold of tf when fetch ngram in second stage
stag2TfFilter=5
stag2CvalueFilter=1
# the thresholh of umls/chv score. no filter if it is -1
stag2UmlsScoreFilter= -1
stag2ChvScoreFilter=-1

# if any of this greater than 0, it will rank the ngram, and take the top N.
# this configuration can affect bags of words function.
topTfNgram=0
topCvalueNgram=0
topTfdfNgram=0

######################## bags of words configuration ###############
# Enable flag
bagsOfWord=false
# only use UMLS OR CHV term as words in bags. Depend on 'trainOnlyChv': true=>CHV, false=>UMLS
bowUmlsOnly=false
bowTfFilter=10
# maximum number of bag of words
bowTopNgram=10000
bowDialogSetOne=true
bowOutputMatrix=true
######################## end of bags of words configuration ######

#################  Metamap configuration ##########################
MMenable=false
# output option have to implement by yourself. don't use as a option.
# -J (--restrict_to_sts) <list>  -e (--exclude_sources) <list>  -R (--restrict_to_sources) <list>
MMoptions=--allow_concept_gaps
MMscoreThreshold = 800
MMhost=
MMport=
# only perform metamap parsing.
MMonly=false
################# end Metamap configuration #######################

#######################################################################
############# Clustering relative configuration #######################
# openNlp do not allow multi-thread, so you can not use local[N] for generating Ngram, but you can use it to run kmeans
# Use stanford nlp will be better
sparkMaster=local
partitionNumber=2
repartitionForce=false

#####*_*####get the training data from (previous save) file, do not construct the Ngram again.
clusteringFromFile=true
# read text from files of a directory, instead of from database
textFromDirectory=false
# the directory of files, if textFromDirectory = true
textDirectory=
# save ngram result to a file.
ngramSaveFile=c:\\fsu\\ra\\data\\ngram_yahoo_0211.serd

########### only use chv term as trainig data
trainOnlyChv=true
# filter the ngran before run kmeans (remove the matched item)
trainedNgramFilterPosRegex=[^N]*PN
# how many percent of the data is sample as test data(for evaluation), <= 0, no thing is test
testSample=30
sampleRuns=1
#number of ngram for training. For test purpose. <0: no limit;
trainNgramCnt=-1

####### if normalized the feature to [0,1] range. see https://en.wikipedia.org/wiki/Feature_scaling
# Rescaling or Standardization
normalizeFeature=true
#rescale to [0,1]
normalize_rescale=true
# the same as z-score. Be sure you know what it affects if you try to use it
normalize_standardize=false
# if the z-score of a value is grater than this factor, it is considered as outlier and will be regular to the value to whose z-score is this factor.
#like a lower boundary and upper boundary
normalize_outlier_factor=2
# generate feature vectors only. So you can use the vectors in R or other tools.
outputVectorOnly=false
# PCA only. Compact the feature space matrix to a N dimensions space using PCA. <=0, do nothing.
pcaDimension=0

# the feature used in kmeans
#           tfdf,cvalue,umls_score,chv_score,contain_umls,contain_chv,win_umls,win_chv,sent_umls,sent_chv,umls_dist,chv_dist,nn,an,pn,anpn,stys,win_pos,capt_first,capt_all,capt_term
useFeatures4Train=tfdf:0,tf,df,cvalue:1,umls_score:0.5,chv_score:0,contain_umls:1,contain_chv:1,nn,an,pn,anpn,stys:0,win_pos,capt_first,capt_all,capt_term,prefix:1,suffix:1,win_umls,win_chv,sent_umls,sent_chv,umls_dist,chv_dist
useFeatures4Test =tfdf:0,tf,df,cvalue:1,umls_score:0.5,chv_score:0,contain_umls:1,contain_chv:1,nn,an,pn,anpn,stys:0,win_pos,capt_first,capt_all,capt_term,prefix:1,suffix:1,win_umls,win_chv,sent_umls,sent_chv,umls_dist,chv_dist
#useFeatures4Train=tfdf,cvalue,umls_score,chv_score,contain_umls,contain_chv,nn,an,pn,anpn,stys,win_pos,capt_first,capt_all,capt_term,prefix,suffix,win_umls,win_chv,sent_umls,sent_chv,umls_dist,chv_dist
#useFeatures4Test =tfdf,cvalue,umls_score,chv_score,contain_umls,contain_chv,nn,an,pn,anpn,stys,win_pos,capt_first,capt_all,capt_term,prefix,suffix,win_umls,win_chv,sent_umls,sent_chv,umls_dist,chv_dist

# the top semantic type we make it as features; only for 'getUmlsScore' function, not 'select'
# for chv paper
semanticType=T033,T121,T061,T047,T109,T023,T184,T074,T116,T123,T059,T046
# for clinical trails pattern paper
#semanticType=T201,T102,T059,T060,T031,T034,T192,T121,T123,T195,T126,T200,T131,T120,T197,T109,T032,T098,T099,T061,T058,T040,T042,T023,T029,T047,T046,T019,T048,T041,T184,T020,T037,T191,T033,T185,T005,T100,T099,T074,T083,T055,T089,T190,T050,T062,
# all semantic type sorted by largest to smallest in size
#semanticType=T116,T020,T052,T100,T087,T011,T190,T008,T017,T195,T194,T123,T007,T031,T022,T053,T038,T012,T029,T091,T122,T023,T030,T118,T026,T043,T025,T019,T103,T120,T104,T185,T201,T200,T077,T049,T088,T060,T056,T203,T047,T065,T069,T111,T196,T050,T018,T071,T126,T204,T051,T099,T021,T013,T033,T004,T168,T169,T045,T083,T028,T064,T102,T096,T068,T093,T058,T131,T125,T016,T078,T129,T055,T197,T037,T170,T130,T171,T059,T034,T119,T015,T063,T066,T074,T041,T073,T048,T044,T085,T191,T114,T070,T124,T086,T057,T090,T115,T109,T032,T040,T001,T092,T042,T046,T072,T067,T039,T121,T002,T101,T098,T097,T094,T080,T081,T192,T014,T062,T075,T089,T167,T095,T054,T184,T082,T110,T024,T079,T061,T005,T127,T010
# filter the semantic type by a regular expression. tag extraction function.
#sabFilter=SNOMEDCT_US|NCI|GO
sabFilter=.*
# the syntax that occur around a ngram in a window. they have been transformed to a single character
posInWindown=CMDEFPANURTVO
# prefix/suffix use window or only process ngram itself
prefixSuffixUseWindow=false
tfdfLessLog=false

###### k-mean parameters #######
# if run k-mean or not
runKmeans=true
# the start/end/step point of the k (cluster number)
k_start=20
k_end=20
k_step=1
# the maximum of iteration of the k-mean algorithm if it is not convergent
maxIterations=1000
# run the following number of times for every k, and take the least cost one
runs=10
# if remove these clusters that contain too small number of points.
reviseModel=true
#################################
# how many percent of the training data does a cluster at least contain (compare to the average number of ngram in a cluster. ?
clusterThresholdPt=10
clusterThresholdLimit=3
# the number of point sampled to calculate the average distance of point to centers
clusterThresholSample=500
# if (the distance of a center to other centers) / (average distance of a point to a center) > (this factor), this center will not be discard.
clusterThresholFactor=3
# get the score for every K(number of cluster), and then we can choose a 'best' K.
#see:https://en.wikipedia.org/wiki/Silhouette_(clustering)
clusterScore=false
#######################################
### Ranking relative configuration ###
# get the baseline result( rank on tfall and cvlaue)
baseLineRank=true
runRank=true
# the granular of rank (percent), e.g. rankGranular=5 work as 5%, 10%, 15%,
rankGranular=5
# since rank are based on percentage of ngram, this parameter specify the base number to calculate the percentage.
# percentage = (# of current ngram)/rankLevelBase, -1: use number of tested ngram if testSample>0, or use number of all ngram
rankLevelBase=-1
# specify how many level of percentage, e.g. 4 means 5%,10%,15%,20%, the percentage may be greater than 100%
rankLevelNumber=40

# the beta value for evaluating f-score, see:https://en.wikipedia.org/wiki/F1_score
# Two other commonly used F measures are the F_{2} measure, which weights recall higher than precision,
#and the F_{0.5} measure, which puts more emphasis on precision than recall.
fscoreBeta=0.5

# ranking with the training data, mainly to evaluatation
rankWithTrainData=false

#######################################################################
############### Output configuration ##################################
# output normalized text for word2vex
outputNormalizedText=false
outputNoCuiSentence=true
#show original ngram before training
showOrgNgramNum=100
showSentence=true
showGroupDesc=false
# save the above showing ngram to file
saveNgram2file=c:/fsu/ra/orgGram.csv
# shown ngram filter based on N
showOrgNgramOfN=1,2,3,4,5
# shown ngram filter based on pos tagger*
showOrgNgramOfPosRegex=.
# shown ngram filter based on text
showOrgNgramOfTextRegex=.*
# show the number of ngram in every cluster. <0, show nothing
showNgramInCluster=0
#show the average and standard deviation of tf in clusters. Not configurable, always true
#showTfAvgSdInCluster=true
#how many percent of ngram is shown the detail after rank. it show info of every ngram in this top ${showDetailRankPt} percent; <0 don't show detail;
showDetailRankPt=0
# if a Ngram math this filter(regex), the detail information will output to console..
debugFilterNgram=aaaaaaaaaaaaaaaaaa
